{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration, testing, visualization\n",
        "\n",
        "## Step 2: Load and visualize point clouds\n",
        "\n",
        "This notebook demonstrates how to load and visualize point clouds using Open3D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add backend to path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "\n",
        "from backend.data.datasets import load_point_cloud, get_point_cloud_info\n",
        "from backend.data.preprocessing import normalize_point_cloud\n",
        "from backend.visualization.visualize_o3d import show_point_cloud\n",
        "import open3d as o3d\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Create and visualize a synthetic point cloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple sphere point cloud for testing\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd = mesh.sample_points_uniformly(number_of_points=2000)\n",
        "\n",
        "# Display info\n",
        "info = get_point_cloud_info(pcd)\n",
        "print(f\"Point cloud info: {info}\")\n",
        "\n",
        "# Normalize\n",
        "pcd_normalized = normalize_point_cloud(pcd)\n",
        "\n",
        "# Visualize\n",
        "show_point_cloud(pcd_normalized, window_name=\"Synthetic Sphere\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Load a point cloud from file\n",
        "\n",
        "To load a .pcd or .ply file, uncomment and modify the path below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and set your file path:\n",
        "# file_path = \"path/to/your/pointcloud.pcd\"  # or .ply\n",
        "# pcd = load_point_cloud(file_path)\n",
        "# pcd_normalized = normalize_point_cloud(pcd)\n",
        "# show_point_cloud(pcd_normalized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preprocessing - Downsampling, Normalization, Partial Views\n",
        "\n",
        "This section demonstrates preprocessing operations on point clouds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from backend.data.preprocessing import (\n",
        "    downsample_voxel,\n",
        "    normalize_point_cloud,\n",
        "    create_partial_point_cloud,\n",
        "    add_gaussian_noise,\n",
        "    mask_points_by_angle\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Downsampling with Voxel Grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dense point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=50)\n",
        "pcd_original = mesh.sample_points_uniformly(number_of_points=10000)\n",
        "\n",
        "print(f\"Original: {len(pcd_original.points)} points\")\n",
        "\n",
        "# Downsample\n",
        "pcd_downsampled = downsample_voxel(pcd_original, voxel_size=0.1)\n",
        "print(f\"Downsampled: {len(pcd_downsampled.points)} points\")\n",
        "\n",
        "# Visualize comparison\n",
        "from backend.visualization.visualize_o3d import show_point_clouds\n",
        "show_point_clouds([pcd_original, pcd_downsampled], window_name=\"Original vs Downsampled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Normalization Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a point cloud with arbitrary scale and position\n",
        "mesh = o3d.geometry.TriangleMesh.create_box(width=5, height=3, depth=2)\n",
        "pcd_original = mesh.sample_points_uniformly(number_of_points=2000)\n",
        "\n",
        "# Apply different normalization methods\n",
        "pcd_unit_sphere = normalize_point_cloud(pcd_original, method='unit_sphere')\n",
        "pcd_centered = normalize_point_cloud(pcd_original, method='centered')\n",
        "pcd_zero_one = normalize_point_cloud(pcd_original, method='zero_one')\n",
        "\n",
        "print(\"Normalization methods applied:\")\n",
        "print(f\"  Original: {np.asarray(pcd_original.points)[0]}\")\n",
        "print(f\"  Unit sphere: {np.asarray(pcd_unit_sphere.points)[0]}\")\n",
        "print(f\"  Centered: {np.asarray(pcd_centered.points)[0]}\")\n",
        "print(f\"  Zero-one: {np.asarray(pcd_zero_one.points)[0]}\")\n",
        "\n",
        "# Visualize\n",
        "show_point_clouds([pcd_original, pcd_unit_sphere], window_name=\"Original vs Normalized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Create Partial Point Cloud (Mask Side)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a complete point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd_complete = mesh.sample_points_uniformly(number_of_points=3000)\n",
        "\n",
        "# Create partial view (mask one side)\n",
        "pcd_partial = create_partial_point_cloud(\n",
        "    pcd_complete,\n",
        "    method='mask_side',\n",
        "    direction=[1, 0, 0],  # Mask points on +X side\n",
        "    mask_ratio=0.5\n",
        ")\n",
        "\n",
        "print(f\"Complete: {len(pcd_complete.points)} points\")\n",
        "print(f\"Partial: {len(pcd_partial.points)} points\")\n",
        "\n",
        "# Visualize\n",
        "show_point_clouds([pcd_complete, pcd_partial], window_name=\"Complete vs Partial\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: Simulate Sensor Field of View (Angle > 45°)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd_complete = mesh.sample_points_uniformly(number_of_points=3000)\n",
        "\n",
        "# Method 1: Using create_partial_point_cloud with angle masking\n",
        "pcd_angle_masked = create_partial_point_cloud(\n",
        "    pcd_complete,\n",
        "    method='mask_angle',\n",
        "    direction=[1, 0, 0],  # Sensor looking along +X\n",
        "    max_angle=45.0  # Keep only points within 45° field of view\n",
        ")\n",
        "\n",
        "# Method 2: Using mask_points_by_angle (same result)\n",
        "pcd_angle_masked2 = mask_points_by_angle(\n",
        "    pcd_complete,\n",
        "    view_direction=[1, 0, 0],\n",
        "    max_angle=45.0\n",
        ")\n",
        "\n",
        "print(f\"Complete: {len(pcd_complete.points)} points\")\n",
        "print(f\"Angle masked (method 1): {len(pcd_angle_masked.points)} points\")\n",
        "print(f\"Angle masked (method 2): {len(pcd_angle_masked2.points)} points\")\n",
        "\n",
        "# Visualize\n",
        "show_point_clouds([pcd_complete, pcd_angle_masked], window_name=\"Complete vs Angle Masked\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 5: Add Gaussian Noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a clean point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd_clean = mesh.sample_points_uniformly(number_of_points=2000)\n",
        "\n",
        "# Add Gaussian noise\n",
        "pcd_noisy = add_gaussian_noise(pcd_clean, std=0.02, relative=True)\n",
        "\n",
        "print(f\"Clean: {len(pcd_clean.points)} points\")\n",
        "print(f\"Noisy: {len(pcd_noisy.points)} points\")\n",
        "\n",
        "# Visualize\n",
        "show_point_clouds([pcd_clean, pcd_noisy], window_name=\"Clean vs Noisy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 6: Complete Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete preprocessing pipeline\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=40)\n",
        "pcd_original = mesh.sample_points_uniformly(number_of_points=5000)\n",
        "\n",
        "print(\"Original:\", len(pcd_original.points), \"points\")\n",
        "\n",
        "# Step 1: Downsample\n",
        "pcd = downsample_voxel(pcd_original, voxel_size=0.05)\n",
        "print(\"After downsampling:\", len(pcd.points), \"points\")\n",
        "\n",
        "# Step 2: Create partial view (simulate sensor)\n",
        "pcd = create_partial_point_cloud(pcd, method='mask_angle', direction=[1, 0, 0], max_angle=45.0)\n",
        "print(\"After angle masking:\", len(pcd.points), \"points\")\n",
        "\n",
        "# Step 3: Add noise (simulate sensor noise)\n",
        "pcd = add_gaussian_noise(pcd, std=0.01, relative=True)\n",
        "print(\"After noise:\", len(pcd.points), \"points\")\n",
        "\n",
        "# Step 4: Normalize\n",
        "pcd = normalize_point_cloud(pcd, method='unit_sphere')\n",
        "print(\"After normalization:\", len(pcd.points), \"points\")\n",
        "\n",
        "# Visualize before and after\n",
        "show_point_clouds([pcd_original, pcd], window_name=\"Before vs After Preprocessing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Classical Methods - Normal Estimation + Surface Reconstruction\n",
        "\n",
        "This section demonstrates classical baseline methods:\n",
        "- PCA-based normal estimation (k-NN + local PCA)\n",
        "- Poisson Surface Reconstruction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from backend.geometry.normals import estimate_normals_pca, get_k_nearest_neighbors, compute_local_pca\n",
        "from backend.geometry.reconstruction import (\n",
        "    poisson_surface_reconstruction,\n",
        "    mesh_to_point_cloud,\n",
        "    reconstruct_and_convert\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Estimate Normals using PCA (k-NN + Local PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd = mesh.sample_points_uniformly(number_of_points=2000)\n",
        "pcd = normalize_point_cloud(pcd, method='unit_sphere')\n",
        "\n",
        "print(f\"Original point cloud: {len(pcd.points)} points\")\n",
        "print(f\"Has normals: {pcd.has_normals()}\")\n",
        "\n",
        "# Estimate normals using PCA\n",
        "pcd_with_normals = estimate_normals_pca(pcd, k=30, orient_normals=True)\n",
        "\n",
        "print(f\"\\nAfter PCA normal estimation:\")\n",
        "print(f\"Has normals: {pcd_with_normals.has_normals()}\")\n",
        "if pcd_with_normals.has_normals():\n",
        "    normals = np.asarray(pcd_with_normals.normals)\n",
        "    print(f\"Normal sample: {normals[0]}\")\n",
        "    print(f\"Normal magnitude: {np.linalg.norm(normals[0]):.4f}\")\n",
        "\n",
        "# Visualize with normals\n",
        "show_point_cloud(pcd_with_normals, window_name=\"Point Cloud with PCA Normals\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Poisson Surface Reconstruction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we have normals\n",
        "if not pcd_with_normals.has_normals():\n",
        "    pcd_with_normals = estimate_normals_pca(pcd, k=30)\n",
        "\n",
        "# Perform Poisson reconstruction\n",
        "print(\"Performing Poisson Surface Reconstruction...\")\n",
        "mesh = poisson_surface_reconstruction(\n",
        "    pcd_with_normals,\n",
        "    depth=9,  # Higher = more detail but slower\n",
        "    scale=1.1\n",
        ")\n",
        "\n",
        "print(f\"Reconstructed mesh:\")\n",
        "print(f\"  Vertices: {len(mesh.vertices)}\")\n",
        "print(f\"  Triangles: {len(mesh.triangles)}\")\n",
        "\n",
        "# Convert mesh back to point cloud for comparison\n",
        "pcd_reconstructed = mesh_to_point_cloud(mesh, number_of_points=len(pcd.points))\n",
        "\n",
        "print(f\"  Reconstructed point cloud: {len(pcd_reconstructed.points)} points\")\n",
        "\n",
        "# Visualize comparison\n",
        "show_point_clouds(\n",
        "    [pcd, pcd_reconstructed],\n",
        "    window_name=\"Original vs Poisson Reconstructed\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Complete Pipeline (Load → Normals → Reconstruct → Visualize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete pipeline demonstration\n",
        "print(\"=\" * 60)\n",
        "print(\"Complete Pipeline: Normal Estimation + Poisson Reconstruction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Load or create point cloud\n",
        "mesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=30)\n",
        "pcd_original = mesh.sample_points_uniformly(number_of_points=2000)\n",
        "pcd_original = normalize_point_cloud(pcd_original, method='unit_sphere')\n",
        "\n",
        "print(f\"\\n1. Original point cloud: {len(pcd_original.points)} points\")\n",
        "\n",
        "# Step 2: Estimate normals using PCA\n",
        "pcd_with_normals = estimate_normals_pca(pcd_original, k=30, orient_normals=True)\n",
        "print(f\"2. Normals estimated: {pcd_with_normals.has_normals()}\")\n",
        "\n",
        "# Step 3: Poisson reconstruction\n",
        "mesh_reconstructed = poisson_surface_reconstruction(\n",
        "    pcd_with_normals,\n",
        "    depth=9,\n",
        "    scale=1.1\n",
        ")\n",
        "print(f\"3. Mesh reconstructed: {len(mesh_reconstructed.vertices)} vertices\")\n",
        "\n",
        "# Step 4: Convert to point cloud for comparison\n",
        "pcd_reconstructed = mesh_to_point_cloud(mesh_reconstructed, number_of_points=len(pcd_original.points))\n",
        "print(f\"4. Converted to point cloud: {len(pcd_reconstructed.points)} points\")\n",
        "\n",
        "# Step 5: Visualize\n",
        "print(\"\\n5. Visualizing results...\")\n",
        "show_point_clouds(\n",
        "    [pcd_original, pcd_reconstructed],\n",
        "    window_name=\"Complete Pipeline: Original vs Reconstructed\"\n",
        ")\n",
        "\n",
        "print(\"\\nPipeline completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Deep Learning Model - Sparse UNet\n",
        "\n",
        "This section demonstrates the Sparse UNet model for point cloud completion.\n",
        "Note: Requires MinkowskiEngine (Linux/WSL2 environment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from backend.models.sparse_unet import SparseUNet, create_sparse_tensor, sparse_tensor_to_point_cloud\n",
        "    from backend.models.losses import ChamferLoss, FScoreLoss, CombinedLoss\n",
        "    import MinkowskiEngine as ME\n",
        "    import torch\n",
        "    MINKOWSKI_AVAILABLE = True\n",
        "    print(\"MinkowskiEngine is available!\")\n",
        "except ImportError as e:\n",
        "    print(f\"MinkowskiEngine not available: {e}\")\n",
        "    print(\"This model requires MinkowskiEngine (Linux/WSL2 environment).\")\n",
        "    MINKOWSKI_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Create and Test Sparse UNet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MINKOWSKI_AVAILABLE:\n",
        "    # Create model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    model = SparseUNet(in_channels=3, out_channels=3, base_channels=32)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Count parameters\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model parameters: {num_params:,}\")\n",
        "    \n",
        "    # Create dummy input\n",
        "    num_points = 1000\n",
        "    coordinates = torch.randint(0, 100, (num_points, 3), dtype=torch.long, device=device)\n",
        "    batch_indices = torch.zeros(num_points, 1, dtype=torch.long, device=device)\n",
        "    coordinates_with_batch = torch.cat([batch_indices, coordinates], dim=1)\n",
        "    features = torch.randn(num_points, 3, device=device)\n",
        "    \n",
        "    input_sparse = ME.SparseTensor(\n",
        "        features=features,\n",
        "        coordinates=coordinates_with_batch,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nInput: {len(input_sparse.F)} points\")\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output_sparse = model(input_sparse)\n",
        "    \n",
        "    print(f\"Output: {len(output_sparse.F)} points\")\n",
        "    print(\"✓ Forward pass successful!\")\n",
        "else:\n",
        "    print(\"Skipping - MinkowskiEngine not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MINKOWSKI_AVAILABLE:\n",
        "    # Create dummy predictions and targets\n",
        "    pred_points = torch.randn(500, 3, device=device)\n",
        "    target_points = torch.randn(800, 3, device=device)\n",
        "    \n",
        "    # Chamfer Distance\n",
        "    chamfer_loss = ChamferLoss()\n",
        "    chamfer_val = chamfer_loss(pred_points, target_points)\n",
        "    print(f\"Chamfer Distance: {chamfer_val.item():.6f}\")\n",
        "    \n",
        "    # F-score\n",
        "    fscore_loss = FScoreLoss(threshold=0.1)\n",
        "    fscore_val = fscore_loss(pred_points, target_points)\n",
        "    print(f\"F-score Loss (1 - F): {fscore_val.item():.6f}\")\n",
        "    \n",
        "    # Combined loss\n",
        "    combined_loss = CombinedLoss(\n",
        "        chamfer_weight=1.0,\n",
        "        fscore_weight=0.5,\n",
        "        normal_weight=0.0\n",
        "    )\n",
        "    combined_val = combined_loss(pred_points, target_points)\n",
        "    print(f\"Combined Loss: {combined_val.item():.6f}\")\n",
        "else:\n",
        "    print(\"Skipping - MinkowskiEngine not available\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
